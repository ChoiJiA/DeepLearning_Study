{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate & Evaluation    \n",
    "\n",
    "- 핵심 : 데이터 셋을 Training/Testing set으로 나눔. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]     # Training set : 학습 시에 사용 \n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]   # Testing set : 학습이 끝난 뒤 성능 평가시 사용. \n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.73203 [[ 0.7288166   0.7153621  -1.1801533 ]\n",
      " [-0.5775373  -0.1298833   1.6072978 ]\n",
      " [ 0.48373488 -0.51433605 -2.02127   ]]\n",
      "1 3.317995 [[ 0.6621908   0.74796313 -1.1461285 ]\n",
      " [-0.81948906  0.03000022  1.689366  ]\n",
      " [ 0.23214608 -0.33772916 -1.9462881 ]]\n",
      "2 2.0218027 [[ 0.6434202   0.7412768  -1.1206716 ]\n",
      " [-0.8116129  -0.00900117  1.7204912 ]\n",
      " [ 0.20866647 -0.35079566 -1.909742  ]]\n",
      "3 1.9710885 [[ 0.6235321   0.7400823  -1.099589  ]\n",
      " [-0.80967706 -0.01636278  1.725917  ]\n",
      " [ 0.17870611 -0.3366575  -1.8939198 ]]\n",
      "4 1.9446772 [[ 0.60733104  0.7366498  -1.0799555 ]\n",
      " [-0.79007834 -0.03363192  1.7235874 ]\n",
      " [ 0.16691463 -0.33406347 -1.8847224 ]]\n",
      "5 1.9235673 [[ 0.59031737  0.73510236 -1.0613943 ]\n",
      " [-0.77496755 -0.04048847  1.7153332 ]\n",
      " [ 0.15076597 -0.32209396 -1.8805432 ]]\n",
      "6 1.9035923 [[ 0.57437915  0.7326285  -1.0429822 ]\n",
      " [-0.75527763 -0.05152     1.7066748 ]\n",
      " [ 0.13977788 -0.31497923 -1.8766699 ]]\n",
      "7 1.88408 [[ 0.5581266   0.73071784 -1.024819  ]\n",
      " [-0.7377567  -0.05928203  1.696916  ]\n",
      " [ 0.12698433 -0.3051834  -1.8736721 ]]\n",
      "8 1.8649302 [[ 0.54225254  0.72846895 -1.006696  ]\n",
      " [-0.7190969  -0.06834842  1.6873226 ]\n",
      " [ 0.11586068 -0.29726508 -1.8704668 ]]\n",
      "9 1.8461237 [[ 0.52631956  0.72638667 -0.9886808 ]\n",
      " [-0.70141983 -0.07617539  1.6774726 ]\n",
      " [ 0.1042026  -0.2886617  -1.8674121 ]]\n",
      "10 1.8276567 [[ 0.5105773   0.72416687 -0.97071874]\n",
      " [-0.68354446 -0.08428247  1.6677043 ]\n",
      " [ 0.09325795 -0.28090802 -1.8642211 ]]\n",
      "11 1.8095273 [[ 0.49487543  0.7219867  -0.9528367 ]\n",
      " [-0.66620725 -0.09177169  1.6578563 ]\n",
      " [ 0.08226089 -0.27309966 -1.8610324 ]]\n",
      "12 1.7917325 [[ 0.4793073   0.7197361  -0.9350179 ]\n",
      " [-0.6489746  -0.09918401  1.648036  ]\n",
      " [ 0.07167412 -0.26578876 -1.8577565 ]]\n",
      "13 1.774271 [[ 0.46381766  0.71748036 -0.9172725 ]\n",
      " [-0.6321231  -0.10618486  1.6381854 ]\n",
      " [ 0.06121073 -0.25863764 -1.8544443 ]]\n",
      "14 1.7571387 [[ 0.4484437   0.71517694 -0.89959514]\n",
      " [-0.61548597 -0.11297937  1.6283427 ]\n",
      " [ 0.05105058 -0.25185686 -1.8510649 ]]\n",
      "15 1.7403319 [[ 0.43316424  0.7128511  -0.88198984]\n",
      " [-0.5991741  -0.11943594  1.6184874 ]\n",
      " [ 0.04107857 -0.24531214 -1.8476377 ]]\n",
      "16 1.7238464 [[ 0.41799513  0.71048534 -0.864455  ]\n",
      " [-0.5831196  -0.12563753  1.6086345 ]\n",
      " [ 0.03136767 -0.23908748 -1.8441514 ]]\n",
      "17 1.7076764 [[ 0.402928    0.70809    -0.84699255]\n",
      " [-0.56736916 -0.13153006  1.5987766 ]\n",
      " [ 0.02186805 -0.23312509 -1.8406141 ]]\n",
      "18 1.6918167 [[ 0.38797012  0.7056575  -0.8296022 ]\n",
      " [-0.5518933  -0.13714989  1.5889205 ]\n",
      " [ 0.01260978 -0.22745904 -1.8370218 ]]\n",
      "19 1.676261 [[ 0.37311813  0.7031923  -0.812285  ]\n",
      " [-0.5367118  -0.14247467  1.5790638 ]\n",
      " [ 0.00356906 -0.2220618  -1.8333783 ]]\n",
      "20 1.6610024 [[ 0.35837558  0.7006911  -0.79504126]\n",
      " [-0.52181023 -0.14752221  1.5692097 ]\n",
      " [-0.00524237 -0.21694617 -1.8296825 ]]\n",
      "21 1.6460342 [[ 0.3437411   0.6981562  -0.7778718 ]\n",
      " [-0.5071964  -0.15228416  1.5593579 ]\n",
      " [-0.01383764 -0.21209693 -1.8259364 ]]\n",
      "22 1.6313493 [[ 0.32921645  0.6955861  -0.760777  ]\n",
      " [-0.49286175 -0.15677102  1.5495101 ]\n",
      " [-0.0222133  -0.20751747 -1.8221402 ]]\n",
      "23 1.6169401 [[ 0.314801    0.69298214 -0.7437576 ]\n",
      " [-0.4788082  -0.16098098  1.5396665 ]\n",
      " [-0.03037806 -0.20319758 -1.8182954 ]]\n",
      "24 1.602799 [[ 0.30049562  0.69034404 -0.7268141 ]\n",
      " [-0.46502936 -0.16492173  1.5298284 ]\n",
      " [-0.03833251 -0.1991361  -1.8144023 ]]\n",
      "25 1.5889187 [[ 0.28629988  0.6876728  -0.7099471 ]\n",
      " [-0.45152396 -0.16859493  1.5199962 ]\n",
      " [-0.04608342 -0.19532496 -1.8104626 ]]\n",
      "26 1.5752912 [[ 0.2722141   0.6849686  -0.69315714]\n",
      " [-0.43828636 -0.17200711  1.5101708 ]\n",
      " [-0.05363356 -0.1917606  -1.8064768 ]]\n",
      "27 1.561909 [[ 0.25823796  0.6822324  -0.67644477]\n",
      " [-0.4253133  -0.17516194  1.5003526 ]\n",
      " [-0.06098878 -0.1884359  -1.8024462 ]]\n",
      "28 1.5487647 [[ 0.24437135  0.67946476 -0.65981054]\n",
      " [-0.41259927 -0.17806564  1.4905423 ]\n",
      " [-0.06815298 -0.185346   -1.7983719 ]]\n",
      "29 1.5358505 [[ 0.23061384  0.6766667  -0.64325494]\n",
      " [-0.40014    -0.18072301  1.4807404 ]\n",
      " [-0.07513174 -0.18248422 -1.7942549 ]]\n",
      "30 1.5231596 [[ 0.21696515  0.67383885 -0.6267784 ]\n",
      " [-0.38792977 -0.18314028  1.4709475 ]\n",
      " [-0.08192948 -0.17984508 -1.7900963 ]]\n",
      "31 1.5106845 [[ 0.20342474  0.67098236 -0.61038154]\n",
      " [-0.3759636  -0.18532285  1.4611639 ]\n",
      " [-0.08855159 -0.177422   -1.7858973 ]]\n",
      "32 1.4984188 [[ 0.18999213  0.66809815 -0.5940647 ]\n",
      " [-0.3642357  -0.18727708  1.4513901 ]\n",
      " [-0.0950028  -0.17520927 -1.7816588 ]]\n",
      "33 1.486355 [[ 0.17666668  0.66518724 -0.57782835]\n",
      " [-0.35274073 -0.18900873  1.4416268 ]\n",
      " [-0.10128839 -0.17320046 -1.777382  ]]\n",
      "34 1.474487 [[ 0.16344777  0.66225064 -0.56167287]\n",
      " [-0.34147277 -0.19052416  1.4318743 ]\n",
      " [-0.10741312 -0.17138971 -1.7730681 ]]\n",
      "35 1.4628086 [[ 0.15033467  0.6592895  -0.5455986 ]\n",
      " [-0.33042628 -0.19182931  1.422133  ]\n",
      " [-0.11338215 -0.16977078 -1.768718  ]]\n",
      "36 1.4513137 [[ 0.13732667  0.65630484 -0.529606  ]\n",
      " [-0.31959537 -0.19293049  1.4124032 ]\n",
      " [-0.11920019 -0.1683378  -1.7643329 ]]\n",
      "37 1.4399967 [[ 0.12442295  0.6532978  -0.51369524]\n",
      " [-0.30897442 -0.19383372  1.4026855 ]\n",
      " [-0.12487219 -0.16708478 -1.7599139 ]]\n",
      "38 1.4288518 [[ 0.1116227   0.65026945 -0.49786666]\n",
      " [-0.29855767 -0.19454522  1.3929802 ]\n",
      " [-0.13040283 -0.16600597 -1.755462  ]]\n",
      "39 1.417874 [[ 0.09892504  0.64722097 -0.48212054]\n",
      " [-0.28833953 -0.19507097  1.3832878 ]\n",
      " [-0.13579683 -0.16509554 -1.7509785 ]]\n",
      "40 1.4070581 [[ 0.08632909  0.6441535  -0.4664571 ]\n",
      " [-0.27831438 -0.19541699  1.3736087 ]\n",
      " [-0.1410587  -0.16434793 -1.7464643 ]]\n",
      "41 1.3963993 [[ 0.07383393  0.64106804 -0.45087653]\n",
      " [-0.26847678 -0.19558915  1.3639433 ]\n",
      " [-0.14619291 -0.1637576  -1.7419204 ]]\n",
      "42 1.3858929 [[ 0.06143863  0.63796586 -0.43537903]\n",
      " [-0.25882128 -0.19559331  1.354292  ]\n",
      " [-0.15120374 -0.16331919 -1.737348  ]]\n",
      "43 1.3755348 [[ 0.04914222  0.63484794 -0.4199647 ]\n",
      " [-0.24934267 -0.19543517  1.3446553 ]\n",
      " [-0.15609546 -0.16302739 -1.732748  ]]\n",
      "44 1.3653209 [[ 0.03694373  0.6317154  -0.4046337 ]\n",
      " [-0.24003571 -0.19512035  1.3350335 ]\n",
      " [-0.16087212 -0.1628771  -1.7281216 ]]\n",
      "45 1.3552469 [[ 0.02484218  0.62856936 -0.3893861 ]\n",
      " [-0.23089539 -0.19465427  1.3254272 ]\n",
      " [-0.1655377  -0.16286324 -1.7234699 ]]\n",
      "46 1.3453094 [[ 0.01283655  0.62541085 -0.37422195]\n",
      " [-0.22191676 -0.1940424   1.3158367 ]\n",
      " [-0.17009604 -0.16298102 -1.7187937 ]]\n",
      "47 1.3355045 [[ 9.2582870e-04  6.2224090e-01 -3.5914129e-01]\n",
      " [-2.1309505e-01 -1.9328994e-01  1.3062625e+00]\n",
      " [-1.7455086e-01 -1.6322561e-01 -1.7140943e+00]]\n",
      "48 1.3258293 [[-0.010891    0.6190606  -0.34414414]\n",
      " [-0.20442562 -0.19240199  1.2967051 ]\n",
      " [-0.17890574 -0.16359241 -1.7093725 ]]\n",
      "49 1.3162804 [[-0.02261495  0.61587083 -0.32923046]\n",
      " [-0.19590388 -0.19138357  1.2871649 ]\n",
      " [-0.1831641  -0.16407697 -1.7046297 ]]\n",
      "50 1.3068547 [[-0.03424708  0.61267275 -0.31440023]\n",
      " [-0.18752554 -0.1902395   1.2776425 ]\n",
      " [-0.18732935 -0.16467485 -1.6998665 ]]\n",
      "51 1.2975492 [[-0.04578841  0.6094672  -0.29965338]\n",
      " [-0.17928627 -0.18897462  1.2681383 ]\n",
      " [-0.19140458 -0.16538191 -1.6950842 ]]\n",
      "52 1.2883614 [[-0.05723997  0.60625523 -0.28498983]\n",
      " [-0.17118199 -0.1875934   1.2586528 ]\n",
      " [-0.19539295 -0.1661939  -1.6902839 ]]\n",
      "53 1.2792885 [[-0.06860282  0.6030377  -0.27040946]\n",
      " [-0.16320875 -0.18610041  1.2491865 ]\n",
      " [-0.19929741 -0.16710693 -1.6854664 ]]\n",
      "54 1.2703285 [[-0.07987796  0.59981555 -0.25591213]\n",
      " [-0.15536267 -0.18449995  1.23974   ]\n",
      " [-0.20312078 -0.16811708 -1.6806328 ]]\n",
      "55 1.2614784 [[-0.09106643  0.5965896  -0.24149771]\n",
      " [-0.14763999 -0.18279625  1.2303137 ]\n",
      " [-0.20686576 -0.1692206  -1.6757843 ]]\n",
      "56 1.2527364 [[-0.1021693   0.5933608  -0.22716603]\n",
      " [-0.14003722 -0.1809934   1.220908  ]\n",
      " [-0.21053502 -0.1704138  -1.6709219 ]]\n",
      "57 1.2441001 [[-0.11318755  0.5901299  -0.2129169 ]\n",
      " [-0.13255084 -0.17909545  1.2115238 ]\n",
      " [-0.214131   -0.17169325 -1.6660465 ]]\n",
      "58 1.2355678 [[-0.12412223  0.5868978  -0.19875011]\n",
      " [-0.12517753 -0.17710616  1.2021612 ]\n",
      " [-0.21765614 -0.17305537 -1.6611593 ]]\n",
      "59 1.2271373 [[-0.13497435  0.58366525 -0.18466544]\n",
      " [-0.11791407 -0.17502935  1.1928209 ]\n",
      " [-0.22111267 -0.1744969  -1.6562612 ]]\n",
      "60 1.2188071 [[-0.14574492  0.5804331  -0.17066267]\n",
      " [-0.1107574  -0.17286864  1.1835035 ]\n",
      " [-0.22450283 -0.17601462 -1.6513534 ]]\n",
      "61 1.210575 [[-0.15643494  0.57720196 -0.15674154]\n",
      " [-0.10370452 -0.17062756  1.1742096 ]\n",
      " [-0.2278287  -0.17760536 -1.6464367 ]]\n",
      "62 1.2024395 [[-0.1670454   0.5739727  -0.1429018 ]\n",
      " [-0.09675252 -0.16830951  1.1649395 ]\n",
      " [-0.23109221 -0.17926607 -1.6415124 ]]\n",
      "63 1.1943991 [[-0.1775773   0.57074594 -0.12914315]\n",
      " [-0.08989868 -0.16591784  1.155694  ]\n",
      " [-0.23429534 -0.18099383 -1.6365815 ]]\n",
      "64 1.1864524 [[-0.18803163  0.5675224  -0.11546531]\n",
      " [-0.08314037 -0.16345574  1.1464736 ]\n",
      " [-0.2374399  -0.18278572 -1.6316451 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 1.1785977 [[-0.19840932  0.5643028  -0.101868  ]\n",
      " [-0.07647492 -0.16092637  1.1372788 ]\n",
      " [-0.24052751 -0.184639   -1.6267042 ]]\n",
      "66 1.1708336 [[-0.20871137  0.5610877  -0.08835089]\n",
      " [-0.06990003 -0.1583327   1.1281103 ]\n",
      " [-0.24355999 -0.18655089 -1.6217599 ]]\n",
      "67 1.1631591 [[-0.21893871  0.55787784 -0.07491367]\n",
      " [-0.06341317 -0.15567777  1.1189685 ]\n",
      " [-0.24653873 -0.18851885 -1.6168132 ]]\n",
      "68 1.1555727 [[-0.2290923   0.5546738  -0.06155602]\n",
      " [-0.05701218 -0.1529643   1.109854  ]\n",
      " [-0.24946535 -0.19054022 -1.6118652 ]]\n",
      "69 1.148073 [[-0.23917305  0.5514761  -0.04827759]\n",
      " [-0.05069481 -0.15019517  1.1007675 ]\n",
      " [-0.25234115 -0.19261262 -1.6069169 ]]\n",
      "70 1.1406591 [[-0.2491819   0.5482854  -0.03507805]\n",
      " [-0.044459   -0.14737299  1.0917095 ]\n",
      " [-0.25516757 -0.19473353 -1.6019696 ]]\n",
      "71 1.1333299 [[-0.25911975  0.54510224 -0.02195704]\n",
      " [-0.03830272 -0.14450039  1.0826806 ]\n",
      " [-0.25794587 -0.19690065 -1.5970242 ]]\n",
      "72 1.1260841 [[-0.26898748  0.54192716 -0.00891422]\n",
      " [-0.03222401 -0.14157994  1.0736815 ]\n",
      " [-0.26067722 -0.19911171 -1.5920818 ]]\n",
      "73 1.1189208 [[-0.278786    0.53876066  0.00405077]\n",
      " [-0.02622103 -0.13861401  1.0647125 ]\n",
      " [-0.26336282 -0.20136443 -1.5871435 ]]\n",
      "74 1.111839 [[-0.2885162   0.5356033   0.01693832]\n",
      " [-0.02029197 -0.13560511  1.0557746 ]\n",
      " [-0.26600373 -0.2036567  -1.5822103 ]]\n",
      "75 1.1048377 [[-0.29817888  0.53245556  0.02974876]\n",
      " [-0.01443514 -0.13255545  1.0468681 ]\n",
      " [-0.268601   -0.20598635 -1.5772834 ]]\n",
      "76 1.0979161 [[-0.30777493  0.5293179   0.04248248]\n",
      " [-0.00864883 -0.12946734  1.0379937 ]\n",
      " [-0.27115554 -0.20835136 -1.5723639 ]]\n",
      "77 1.0910733 [[-0.3173052   0.5261908   0.05513985]\n",
      " [-0.00293155 -0.12634291  1.0291519 ]\n",
      " [-0.2736684  -0.21074967 -1.5674527 ]]\n",
      "78 1.0843081 [[-0.3267705   0.52307475  0.06772126]\n",
      " [ 0.00271832 -0.12318441  1.0203435 ]\n",
      " [-0.27614033 -0.21317945 -1.562551  ]]\n",
      "79 1.0776201 [[-0.3361717   0.5199701   0.08022706]\n",
      " [ 0.00830215 -0.11999379  1.011569  ]\n",
      " [-0.27857226 -0.21563867 -1.5576599 ]]\n",
      "80 1.0710086 [[-0.3455095   0.51687735  0.09265766]\n",
      " [ 0.01382142 -0.11677314  1.0028291 ]\n",
      " [-0.28096488 -0.21812554 -1.5527804 ]]\n",
      "81 1.0644722 [[-0.3547848   0.51379687  0.10501342]\n",
      " [ 0.01927742 -0.11352435  0.9941243 ]\n",
      " [-0.28331903 -0.2206382  -1.5479136 ]]\n",
      "82 1.0580108 [[-0.3639983   0.5107291   0.11729473]\n",
      " [ 0.02467153 -0.11024942  0.9854553 ]\n",
      " [-0.28563523 -0.22317494 -1.5430605 ]]\n",
      "83 1.0516233 [[-0.3731508   0.50767434  0.12950197]\n",
      " [ 0.03000485 -0.10695008  0.9768226 ]\n",
      " [-0.28791434 -0.22573395 -1.5382224 ]]\n",
      "84 1.0453093 [[-0.38224304  0.504633    0.1416355 ]\n",
      " [ 0.0352787  -0.10362829  0.96822697]\n",
      " [-0.2901568  -0.22831368 -1.5334003 ]]\n",
      "85 1.039068 [[-0.39127576  0.5016055   0.15369575]\n",
      " [ 0.04049411 -0.10028567  0.95966893]\n",
      " [-0.29236335 -0.23091233 -1.5285951 ]]\n",
      "86 1.0328984 [[-0.4002497   0.49859214  0.16568306]\n",
      " [ 0.04565227 -0.09692407  0.95114917]\n",
      " [-0.29453436 -0.23352845 -1.523808  ]]\n",
      "87 1.0268005 [[-0.40916556  0.49559325  0.17759782]\n",
      " [ 0.05075411 -0.09354501  0.94266826]\n",
      " [-0.2966705  -0.23616031 -1.51904   ]]\n",
      "88 1.0207732 [[-0.41802406  0.49260917  0.18944041]\n",
      " [ 0.05580078 -0.09015029  0.9342269 ]\n",
      " [-0.2987721  -0.23880655 -1.5142921 ]]\n",
      "89 1.0148159 [[-0.4268259   0.48964024  0.2012112 ]\n",
      " [ 0.0607931  -0.08674133  0.9258256 ]\n",
      " [-0.30083975 -0.24146551 -1.5095655 ]]\n",
      "90 1.0089284 [[-0.43557176  0.48668674  0.21291056]\n",
      " [ 0.06573213 -0.0833198   0.91746503]\n",
      " [-0.3028737  -0.24413587 -1.5048611 ]]\n",
      "91 1.0031098 [[-0.44426233  0.48374897  0.22453888]\n",
      " [ 0.0706186  -0.0798871   0.90914583]\n",
      " [-0.3048745  -0.24681607 -1.5001801 ]]\n",
      "92 0.9973597 [[-0.4528982   0.4808272   0.23609652]\n",
      " [ 0.07545349 -0.07644482  0.90086865]\n",
      " [-0.30684242 -0.24950483 -1.4955235 ]]\n",
      "93 0.9916775 [[-0.46148008  0.47792178  0.24758384]\n",
      " [ 0.08023752 -0.07299431  0.8926341 ]\n",
      " [-0.30877784 -0.25220072 -1.4908922 ]]\n",
      "94 0.98606277 [[-0.47000858  0.47503293  0.25900123]\n",
      " [ 0.08497152 -0.06953702  0.8844428 ]\n",
      " [-0.31068105 -0.25490245 -1.4862872 ]]\n",
      "95 0.98051465 [[-0.47848433  0.47216088  0.27034903]\n",
      " [ 0.08965618 -0.06607427  0.8762954 ]\n",
      " [-0.31255236 -0.25760865 -1.4817097 ]]\n",
      "96 0.97503304 [[-0.48690796  0.4693059   0.2816276 ]\n",
      " [ 0.0942923  -0.06260746  0.86819243]\n",
      " [-0.31439197 -0.26031813 -1.4771606 ]]\n",
      "97 0.96961737 [[-0.49528003  0.46646827  0.2928373 ]\n",
      " [ 0.09888045 -0.0591378   0.8601346 ]\n",
      " [-0.31620023 -0.26302958 -1.4726409 ]]\n",
      "98 0.9642669 [[-0.50360113  0.4636482   0.3039785 ]\n",
      " [ 0.10342135 -0.05566662  0.85212255]\n",
      " [-0.3179773  -0.2657418  -1.4681516 ]]\n",
      "99 0.95898134 [[-0.5118719   0.46084592  0.31505156]\n",
      " [ 0.10791562 -0.05219515  0.8441568 ]\n",
      " [-0.31972343 -0.26845363 -1.4636936 ]]\n",
      "100 0.95376027 [[-0.52009284  0.45806164  0.3260568 ]\n",
      " [ 0.11236382 -0.04872458  0.836238  ]\n",
      " [-0.3214388  -0.27116385 -1.459268  ]]\n",
      "101 0.94860303 [[-0.5282646   0.4552956   0.3369946 ]\n",
      " [ 0.11676655 -0.04525611  0.8283668 ]\n",
      " [-0.32312357 -0.27387136 -1.4548757 ]]\n",
      "102 0.9435092 [[-0.5363876   0.45254797  0.34786525]\n",
      " [ 0.12112437 -0.04179093  0.8205438 ]\n",
      " [-0.3247779  -0.27657512 -1.4505177 ]]\n",
      "103 0.93847847 [[-0.5444625   0.44981897  0.35866916]\n",
      " [ 0.12543769 -0.03833006  0.81276965]\n",
      " [-0.32640207 -0.2792739  -1.4461946 ]]\n",
      "104 0.93351024 [[-0.55248976  0.44710875  0.36940664]\n",
      " [ 0.12970719 -0.0348747   0.8050448 ]\n",
      " [-0.32799602 -0.2819668  -1.4419078 ]]\n",
      "105 0.9286041 [[-0.5604699   0.44441754  0.38007802]\n",
      " [ 0.1339332  -0.03142583  0.7973699 ]\n",
      " [-0.32956    -0.28465268 -1.437658  ]]\n",
      "106 0.9237596 [[-0.5684035   0.4417455   0.39068365]\n",
      " [ 0.13811624 -0.02798458  0.7897456 ]\n",
      " [-0.3310941  -0.2873306  -1.4334459 ]]\n",
      "107 0.9189762 [[-0.57629097  0.43909276  0.40122384]\n",
      " [ 0.1422567  -0.02455189  0.7821725 ]\n",
      " [-0.3325984  -0.28999954 -1.4292727 ]]\n",
      "108 0.91425365 [[-0.58413285  0.4364595   0.41169894]\n",
      " [ 0.14635505 -0.02112879  0.77465105]\n",
      " [-0.33407298 -0.29265857 -1.4251391 ]]\n",
      "109 0.90959144 [[-0.5919296   0.4338459   0.42210928]\n",
      " [ 0.15041165 -0.01771626  0.76718193]\n",
      " [-0.33551797 -0.29530677 -1.4210459 ]]\n",
      "110 0.9049889 [[-0.59968174  0.43125212  0.43245518]\n",
      " [ 0.1544269  -0.01431522  0.7597656 ]\n",
      " [-0.33693343 -0.29794323 -1.416994  ]]\n",
      "111 0.90044594 [[-0.6073897   0.42867824  0.44273698]\n",
      " [ 0.15840113 -0.01092657  0.7524027 ]\n",
      " [-0.33831945 -0.30056703 -1.4129841 ]]\n",
      "112 0.8959619 [[-0.6150539   0.42612442  0.452955  ]\n",
      " [ 0.16233476 -0.00755126  0.74509376]\n",
      " [-0.33967605 -0.3031774  -1.4090172 ]]\n",
      "113 0.8915362 [[-0.6226748   0.4235908   0.46310958]\n",
      " [ 0.16622803 -0.00419008  0.7378393 ]\n",
      " [-0.34100333 -0.30577338 -1.4050939 ]]\n",
      "114 0.88716865 [[-6.3025296e-01  4.2107752e-01  4.7320101e-01]\n",
      " [ 1.7008135e-01 -8.4394868e-04  7.3063982e-01]\n",
      " [-3.4230128e-01 -3.0835429e-01 -1.4012151e+00]]\n",
      "115 0.88285875 [[-0.6377887   0.41858464  0.48322964]\n",
      " [ 0.17389494  0.00248636  0.7234959 ]\n",
      " [-0.34357002 -0.31091928 -1.3973813 ]]\n",
      "116 0.87860584 [[-0.6452825   0.4161123   0.49319574]\n",
      " [ 0.17766917  0.00580003  0.716408  ]\n",
      " [-0.34480953 -0.31346762 -1.3935934 ]]\n",
      "117 0.8744097 [[-0.65273476  0.4136606   0.5030997 ]\n",
      " [ 0.18140428  0.00909631  0.70937663]\n",
      " [-0.3460199  -0.31599855 -1.3898522 ]]\n",
      "118 0.87026966 [[-0.6601458   0.4112296   0.5129418 ]\n",
      " [ 0.18510056  0.01237443  0.70240223]\n",
      " [-0.34720114 -0.3185114  -1.3861581 ]]\n",
      "119 0.86618555 [[-0.6675162   0.40881944  0.5227223 ]\n",
      " [ 0.18875824  0.01563365  0.6954853 ]\n",
      " [-0.3483533  -0.32100546 -1.3825119 ]]\n",
      "120 0.8621567 [[-0.67484623  0.40643018  0.5324416 ]\n",
      " [ 0.1923776   0.01887331  0.6886263 ]\n",
      " [-0.34947643 -0.32348004 -1.3789141 ]]\n",
      "121 0.85818255 [[-0.68213636  0.40406188  0.5421    ]\n",
      " [ 0.19595888  0.02209267  0.68182564]\n",
      " [-0.3505705  -0.32593456 -1.3753655 ]]\n",
      "122 0.85426277 [[-0.6893869   0.40171462  0.55169785]\n",
      " [ 0.1995023   0.02529109  0.6750838 ]\n",
      " [-0.35163563 -0.3283684  -1.3718666 ]]\n",
      "123 0.85039675 [[-0.6965983   0.3993885   0.56123537]\n",
      " [ 0.20300809  0.02846796  0.6684012 ]\n",
      " [-0.35267183 -0.33078095 -1.3684179 ]]\n",
      "124 0.8465842 [[-0.7037709   0.39708352  0.570713  ]\n",
      " [ 0.2064765   0.03162263  0.6617781 ]\n",
      " [-0.35367906 -0.33317167 -1.3650199 ]]\n",
      "125 0.84282446 [[-0.7109051   0.39479977  0.58013093]\n",
      " [ 0.20990765  0.03475459  0.65521497]\n",
      " [-0.3546575  -0.33553994 -1.3616732 ]]\n",
      "126 0.83911705 [[-0.71800125  0.39253727  0.5894896 ]\n",
      " [ 0.21330191  0.03786312  0.64871216]\n",
      " [-0.355607   -0.33788544 -1.3583783 ]]\n",
      "127 0.83546144 [[-0.72505975  0.39029607  0.5987893 ]\n",
      " [ 0.21665926  0.04094786  0.6422701 ]\n",
      " [-0.35652784 -0.34020746 -1.3551354 ]]\n",
      "128 0.83185726 [[-0.7320809   0.38807622  0.60803026]\n",
      " [ 0.21998015  0.04400811  0.63588893]\n",
      " [-0.3574198  -0.34250572 -1.3519452 ]]\n",
      "129 0.8283038 [[-0.73906505  0.38587773  0.6172129 ]\n",
      " [ 0.22326453  0.04704358  0.62956905]\n",
      " [-0.35828316 -0.34477955 -1.3488079 ]]\n",
      "130 0.8248006 [[-0.74601257  0.3837006   0.6263375 ]\n",
      " [ 0.22651279  0.05005357  0.6233108 ]\n",
      " [-0.3591178  -0.3470288  -1.345724  ]]\n",
      "131 0.82134736 [[-0.7529238   0.38154492  0.63540447]\n",
      " [ 0.22972499  0.0530378   0.61711437]\n",
      " [-0.3599239  -0.34925285 -1.3426938 ]]\n",
      "132 0.8179431 [[-0.75979906  0.3794106   0.64441407]\n",
      " [ 0.2329014   0.0559957   0.6109801 ]\n",
      " [-0.36070144 -0.35145155 -1.3397176 ]]\n",
      "133 0.81458753 [[-0.76663876  0.3772977   0.6533666 ]\n",
      " [ 0.23604208  0.05892701  0.6049081 ]\n",
      " [-0.36145058 -0.35362434 -1.3367957 ]]\n",
      "134 0.81128025 [[-0.7734431   0.3752062   0.6622625 ]\n",
      " [ 0.23914737  0.06183119  0.59889865]\n",
      " [-0.36217126 -0.35577106 -1.3339282 ]]\n",
      "135 0.8080205 [[-0.7802125   0.37313613  0.671102  ]\n",
      " [ 0.24221732  0.06470799  0.5929519 ]\n",
      " [-0.36286366 -0.35789135 -1.3311156 ]]\n",
      "136 0.8048078 [[-0.7869473   0.37108743  0.67988545]\n",
      " [ 0.24525216  0.06755702  0.587068  ]\n",
      " [-0.3635278  -0.35998493 -1.3283578 ]]\n",
      "137 0.80164135 [[-0.79364777  0.36906013  0.68861324]\n",
      " [ 0.24825203  0.070378    0.58124715]\n",
      " [-0.36416382 -0.36205155 -1.3256552 ]]\n",
      "138 0.79852104 [[-0.80031425  0.36705413  0.69728565]\n",
      " [ 0.25121722  0.07317052  0.57548946]\n",
      " [-0.3647717  -0.36409107 -1.3230078 ]]\n",
      "139 0.795446 [[-0.80694705  0.36506948  0.7059031 ]\n",
      " [ 0.25414774  0.07593447  0.569795  ]\n",
      " [-0.3653517  -0.36610317 -1.3204157 ]]\n",
      "140 0.7924156 [[-0.8135464   0.3631061   0.71446586]\n",
      " [ 0.25704396  0.07866947  0.5641638 ]\n",
      " [-0.36590376 -0.36808777 -1.3178791 ]]\n",
      "141 0.78942955 [[-0.82011276  0.36116397  0.7229743 ]\n",
      " [ 0.25990584  0.08137539  0.558596  ]\n",
      " [-0.36642817 -0.3700446  -1.3153979 ]]\n",
      "142 0.786487 [[-0.8266463   0.35924304  0.7314288 ]\n",
      " [ 0.26273376  0.08405191  0.5530916 ]\n",
      " [-0.36692485 -0.37197363 -1.3129721 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 0.7835874 [[-0.8331474   0.35734326  0.7398297 ]\n",
      " [ 0.26552778  0.08669894  0.5476505 ]\n",
      " [-0.3673941  -0.3738747  -1.3106018 ]]\n",
      "144 0.78073025 [[-0.83961636  0.35546455  0.74817735]\n",
      " [ 0.26828814  0.08931624  0.54227287]\n",
      " [-0.36783594 -0.37574774 -1.3082869 ]]\n",
      "145 0.7779149 [[-0.8460534   0.35360688  0.7564721 ]\n",
      " [ 0.27101502  0.09190372  0.5369585 ]\n",
      " [-0.36825052 -0.37759265 -1.3060274 ]]\n",
      "146 0.7751408 [[-0.8524589   0.35177016  0.7647143 ]\n",
      " [ 0.2737086   0.09446116  0.53170747]\n",
      " [-0.368638   -0.37940946 -1.3038231 ]]\n",
      "147 0.7724074 [[-0.85883313  0.34995434  0.77290434]\n",
      " [ 0.27636904  0.09698859  0.5265196 ]\n",
      " [-0.36899862 -0.38119805 -1.3016739 ]]\n",
      "148 0.76971394 [[-0.8651764   0.34815934  0.7810426 ]\n",
      " [ 0.27899665  0.09948579  0.5213948 ]\n",
      " [-0.36933234 -0.3829585  -1.2995797 ]]\n",
      "149 0.76706 [[-0.8714889   0.3463851   0.7891294 ]\n",
      " [ 0.28159145  0.10195282  0.516333  ]\n",
      " [-0.36963955 -0.3846907  -1.2975403 ]]\n",
      "150 0.7644448 [[-0.877771    0.3446315   0.7971651 ]\n",
      " [ 0.28415382  0.1043895   0.51133394]\n",
      " [-0.3699202  -0.38639483 -1.2955555 ]]\n",
      "151 0.7618678 [[-0.884023    0.34289846  0.8051501 ]\n",
      " [ 0.28668383  0.10679594  0.5063975 ]\n",
      " [-0.37017465 -0.38807082 -1.293625  ]]\n",
      "152 0.7593285 [[-0.89024514  0.3411859   0.8130848 ]\n",
      " [ 0.28918177  0.10917202  0.5015235 ]\n",
      " [-0.37040296 -0.38971883 -1.2917486 ]]\n",
      "153 0.7568262 [[-0.8964377   0.33949372  0.8209695 ]\n",
      " [ 0.2916478   0.11151779  0.4967117 ]\n",
      " [-0.37060538 -0.39133888 -1.2899262 ]]\n",
      "154 0.7543605 [[-0.90260094  0.3378218   0.8288047 ]\n",
      " [ 0.29408213  0.11383326  0.4919619 ]\n",
      " [-0.37078208 -0.3929311  -1.2881572 ]]\n",
      "155 0.7519305 [[-0.9087352   0.33617008  0.83659065]\n",
      " [ 0.296485    0.11611848  0.4872738 ]\n",
      " [-0.37093326 -0.39449567 -1.2864414 ]]\n",
      "156 0.74953574 [[-0.9148407   0.33453843  0.8443278 ]\n",
      " [ 0.29885662  0.11837354  0.48264712]\n",
      " [-0.37105918 -0.39603263 -1.2847786 ]]\n",
      "157 0.7471757 [[-0.92091775  0.33292672  0.8520165 ]\n",
      " [ 0.30119723  0.12059844  0.4780816 ]\n",
      " [-0.37115997 -0.3975422  -1.2831682 ]]\n",
      "158 0.74484974 [[-0.92696655  0.3313349   0.85965717]\n",
      " [ 0.303507    0.12279338  0.4735769 ]\n",
      " [-0.37123594 -0.3990245  -1.2816099 ]]\n",
      "159 0.7425573 [[-0.93298745  0.32976276  0.86725014]\n",
      " [ 0.30578628  0.12495834  0.46913266]\n",
      " [-0.3712872  -0.4004798  -1.2801033 ]]\n",
      "160 0.7402977 [[-0.9389807   0.32821026  0.87479585]\n",
      " [ 0.30803517  0.12709357  0.46474853]\n",
      " [-0.3713141  -0.40190813 -1.2786481 ]]\n",
      "161 0.73807037 [[-0.9449465   0.32667723  0.8822947 ]\n",
      " [ 0.310254    0.12919907  0.4604242 ]\n",
      " [-0.3713168  -0.40330988 -1.2772437 ]]\n",
      "162 0.7358748 [[-0.95088524  0.32516357  0.889747  ]\n",
      " [ 0.3124429   0.13127513  0.45615923]\n",
      " [-0.37129557 -0.40468514 -1.2758896 ]]\n",
      "163 0.73371047 [[-0.956797    0.32366914  0.89715326]\n",
      " [ 0.3146023   0.13332179  0.4519532 ]\n",
      " [-0.37125057 -0.40603426 -1.2745855 ]]\n",
      "164 0.7315767 [[-0.9626822   0.32219383  0.9045138 ]\n",
      " [ 0.3167322   0.13533935  0.44780573]\n",
      " [-0.3711822  -0.40735736 -1.2733308 ]]\n",
      "165 0.729473 [[-0.96854097  0.32073748  0.91182894]\n",
      " [ 0.31883305  0.13732785  0.44371638]\n",
      " [-0.37109056 -0.40865484 -1.272125  ]]\n",
      "166 0.72739875 [[-0.9743737   0.31929997  0.91909915]\n",
      " [ 0.32090497  0.13928764  0.43968466]\n",
      " [-0.37097603 -0.4099268  -1.2709676 ]]\n",
      "167 0.7253534 [[-0.98018056  0.31788114  0.92632484]\n",
      " [ 0.32294837  0.14121878  0.43571013]\n",
      " [-0.37083873 -0.4111737  -1.269858  ]]\n",
      "168 0.7233366 [[-0.9859618   0.31648088  0.93350637]\n",
      " [ 0.32496333  0.14312167  0.4317923 ]\n",
      " [-0.37067908 -0.41239563 -1.2687957 ]]\n",
      "169 0.7213476 [[-0.9917177   0.31509903  0.9406441 ]\n",
      " [ 0.32695028  0.14499637  0.42793065]\n",
      " [-0.3704972  -0.41359305 -1.2677802 ]]\n",
      "170 0.71938586 [[-0.9974485   0.31373546  0.94773847]\n",
      " [ 0.32890934  0.14684325  0.42412472]\n",
      " [-0.37029344 -0.41476613 -1.2668109 ]]\n",
      "171 0.717451 [[-1.0031544   0.31239     0.9547898 ]\n",
      " [ 0.33084083  0.14866245  0.42037404]\n",
      " [-0.37006804 -0.41591528 -1.2658871 ]]\n",
      "172 0.7155423 [[-1.0088357   0.31106254  0.9617986 ]\n",
      " [ 0.33274502  0.15045431  0.41667798]\n",
      " [-0.36982128 -0.41704074 -1.2650084 ]]\n",
      "173 0.7136595 [[-1.0144926   0.3097529   0.9687652 ]\n",
      " [ 0.33462217  0.15221904  0.41303608]\n",
      " [-0.36955342 -0.41814288 -1.2641741 ]]\n",
      "174 0.7118019 [[-1.0201255   0.30846098  0.97568995]\n",
      " [ 0.33647257  0.15395698  0.40944776]\n",
      " [-0.36926478 -0.419222   -1.2633836 ]]\n",
      "175 0.70996916 [[-1.0257344   0.3071866   0.9825733 ]\n",
      " [ 0.33829647  0.15566835  0.4059125 ]\n",
      " [-0.36895555 -0.42027846 -1.2626364 ]]\n",
      "176 0.70816064 [[-1.0313197   0.3059296   0.9894155 ]\n",
      " [ 0.34009415  0.15735345  0.4024297 ]\n",
      " [-0.3686261  -0.42131257 -1.2619318 ]]\n",
      "177 0.7063759 [[-1.0368816   0.30468985  0.99621713]\n",
      " [ 0.3418659   0.15901259  0.39899883]\n",
      " [-0.36827666 -0.42232466 -1.2612691 ]]\n",
      "178 0.70461464 [[-1.0424203   0.30346718  1.0029784 ]\n",
      " [ 0.34361202  0.16064605  0.39561927]\n",
      " [-0.3679075  -0.4233151  -1.2606478 ]]\n",
      "179 0.70287603 [[-1.047936    0.30226147  1.0097    ]\n",
      " [ 0.34533268  0.1622542   0.39229047]\n",
      " [-0.36751896 -0.4242842  -1.2600672 ]]\n",
      "180 0.7011599 [[-1.053429    0.30107254  1.0163819 ]\n",
      " [ 0.3470283   0.16383722  0.38901186]\n",
      " [-0.36711124 -0.42523238 -1.2595267 ]]\n",
      "181 0.69946575 [[-1.0588995   0.29990023  1.0230247 ]\n",
      " [ 0.34869903  0.16539557  0.38578278]\n",
      " [-0.3666847  -0.42615986 -1.2590258 ]]\n",
      "182 0.6977931 [[-1.0643477   0.2987444   1.0296288 ]\n",
      " [ 0.35034525  0.16692941  0.38260272]\n",
      " [-0.36623952 -0.42706713 -1.2585638 ]]\n",
      "183 0.6961416 [[-1.0697739   0.29760492  1.0361944 ]\n",
      " [ 0.35196713  0.16843918  0.37947106]\n",
      " [-0.36577612 -0.42795447 -1.2581398 ]]\n",
      "184 0.6945107 [[-1.0751783   0.29648158  1.0427221 ]\n",
      " [ 0.35356507  0.16992514  0.37638715]\n",
      " [-0.36529467 -0.42882225 -1.2577535 ]]\n",
      "185 0.6929 [[-1.080561    0.29537427  1.0492122 ]\n",
      " [ 0.35513932  0.17138763  0.3733504 ]\n",
      " [-0.36479548 -0.4296708  -1.2574041 ]]\n",
      "186 0.6913092 [[-1.0859225   0.29428285  1.055665  ]\n",
      " [ 0.3566901   0.172827    0.37036023]\n",
      " [-0.36427882 -0.4305005  -1.257091  ]]\n",
      "187 0.6897378 [[-1.0912627   0.29320714  1.062081  ]\n",
      " [ 0.3582178   0.17424354  0.36741602]\n",
      " [-0.36374497 -0.43131173 -1.2568136 ]]\n",
      "188 0.68818545 [[-1.0965819   0.29214698  1.0684605 ]\n",
      " [ 0.35972255  0.17563763  0.36451718]\n",
      " [-0.3631943  -0.43210477 -1.2565713 ]]\n",
      "189 0.68665177 [[-1.1018804   0.29110223  1.0748037 ]\n",
      " [ 0.36120483  0.1770095   0.36166304]\n",
      " [-0.36262685 -0.43288007 -1.2563634 ]]\n",
      "190 0.68513626 [[-1.1071584   0.29007277  1.0811112 ]\n",
      " [ 0.36266467  0.17835963  0.35885307]\n",
      " [-0.3620432  -0.43363783 -1.2561892 ]]\n",
      "191 0.6836387 [[-1.1124161   0.2890584   1.0873833 ]\n",
      " [ 0.3641026   0.17968817  0.35608658]\n",
      " [-0.36144337 -0.4343786  -1.2560483 ]]\n",
      "192 0.6821587 [[-1.1176537   0.28805897  1.0936203 ]\n",
      " [ 0.36551872  0.18099561  0.35336304]\n",
      " [-0.3608278  -0.43510255 -1.25594   ]]\n",
      "193 0.68069583 [[-1.1228714   0.28707436  1.0998225 ]\n",
      " [ 0.36691338  0.18228218  0.35068178]\n",
      " [-0.36019668 -0.43581012 -1.2558635 ]]\n",
      "194 0.67924976 [[-1.1280694   0.2861044   1.1059904 ]\n",
      " [ 0.36828685  0.18354824  0.34804225]\n",
      " [-0.35955027 -0.43650162 -1.2558185 ]]\n",
      "195 0.67782027 [[-1.1332479   0.28514895  1.1121243 ]\n",
      " [ 0.36963943  0.18479411  0.34544381]\n",
      " [-0.35888883 -0.43717742 -1.2558041 ]]\n",
      "196 0.676407 [[-1.138407    0.28420785  1.1182245 ]\n",
      " [ 0.37097135  0.18602017  0.34288585]\n",
      " [-0.35821268 -0.4378378  -1.2558198 ]]\n",
      "197 0.67500937 [[-1.143547    0.28328097  1.1242914 ]\n",
      " [ 0.3722829   0.1872267   0.34036776]\n",
      " [-0.35752207 -0.43848315 -1.2558651 ]]\n",
      "198 0.6736274 [[-1.1486682   0.28236815  1.1303253 ]\n",
      " [ 0.37357438  0.18841399  0.337889  ]\n",
      " [-0.35681722 -0.4391138  -1.2559394 ]]\n",
      "199 0.67226064 [[-1.1537706   0.28146926  1.1363266 ]\n",
      " [ 0.37484598  0.18958244  0.33544892]\n",
      " [-0.3560984  -0.43973    -1.2560419 ]]\n",
      "200 0.6709087 [[-1.1588544   0.28058413  1.1422955 ]\n",
      " [ 0.3760981   0.1907323   0.33304697]\n",
      " [-0.35536587 -0.44033217 -1.2561723 ]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "        # Prediction / Accuracy를 측정할 경우에 Test데이터 셋을 사용. \n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate 설정의 중요성\n",
    "- Large Learning Rate(Overshooting) : 값에 수렴하지 않고 발산함.    \n",
    "- Small Learning Rate : 학습이 안될수 있고, local minimum에 빠질 가능성 존재. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  2519696000000.0 \n",
      "Prediction:\n",
      " [[1120640. ]\n",
      " [2254801.8]\n",
      " [1774008. ]\n",
      " [1243901.9]\n",
      " [1465808. ]\n",
      " [1478132.2]\n",
      " [1354844.1]\n",
      " [1724677.5]]\n",
      "1 Cost:  2.7683398e+27 \n",
      "Prediction:\n",
      " [[-3.7114276e+13]\n",
      " [-7.4714840e+13]\n",
      " [-5.8775474e+13]\n",
      " [-4.1201294e+13]\n",
      " [-4.8557927e+13]\n",
      " [-4.8966629e+13]\n",
      " [-4.4879611e+13]\n",
      " [-5.7140669e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[1.2302029e+21]\n",
      " [2.4765244e+21]\n",
      " [1.9481925e+21]\n",
      " [1.3656725e+21]\n",
      " [1.6095181e+21]\n",
      " [1.6230651e+21]\n",
      " [1.4875953e+21]\n",
      " [1.8940044e+21]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[-4.0776733e+28]\n",
      " [-8.2087735e+28]\n",
      " [-6.4575462e+28]\n",
      " [-4.5267056e+28]\n",
      " [-5.3349646e+28]\n",
      " [-5.3798682e+28]\n",
      " [-4.9308354e+28]\n",
      " [-6.2779334e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[1.3516000e+36]\n",
      " [2.7209090e+36]\n",
      " [2.1404410e+36]\n",
      " [1.5004378e+36]\n",
      " [1.7683461e+36]\n",
      " [1.7832299e+36]\n",
      " [1.6343921e+36]\n",
      " [2.0809058e+36]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 Cost:  2.45533e+12\\nPrediction:\\n [[-1104436.375]\\n [-2224342.75 ]\\n [-1749606.75 ]\\n [-1226179.375]\\n [-1445287.125]\\n [-1457459.5  ]\\n [-1335740.5  ]\\n [-1700924.625]]\\n1 Cost:  2.69762e+27\\nPrediction:\\n [[  3.66371490e+13]\\n [  7.37543360e+13]\\n [  5.80198785e+13]\\n [  4.06716290e+13]\\n [  4.79336847e+13]\\n [  4.83371348e+13]\\n [  4.43026590e+13]\\n [  5.64060907e+13]]\\n2 Cost:  inf\\nPrediction:\\n [[ -1.21438790e+21]\\n [ -2.44468702e+21]\\n [ -1.92314724e+21]\\n [ -1.34811610e+21]\\n [ -1.58882674e+21]\\n [ -1.60219962e+21]\\n [ -1.46847142e+21]\\n [ -1.86965602e+21]]\\n3 Cost:  inf\\nPrediction:\\n [[  4.02525216e+28]\\n [  8.10324465e+28]\\n [  6.37453079e+28]\\n [  4.46851237e+28]\\n [  5.26638074e+28]\\n [  5.31070676e+28]\\n [  4.86744608e+28]\\n [  6.19722623e+28]]\\n4 Cost:  inf\\nPrediction:\\n [[ -1.33422428e+36]\\n [ -2.68593010e+36]\\n [ -2.11292430e+36]\\n [ -1.48114879e+36]\\n [ -1.74561303e+36]\\n [ -1.76030542e+36]\\n [ -1.61338091e+36]\\n [ -2.05415459e+36]]\\n5 Cost:  inf\\nPrediction:\\n [[ inf]\\n [ inf]\\n [ inf]\\n [ inf]\\n [ inf]\\n [ inf]\\n [ inf]\\n [ inf]]\\n6 Cost:  nan\\nPrediction:\\n [[ nan]\\n [ nan]\\n [ nan]\\n [ nan]\\n [ nan]\\n [ nan]\\n [ nan]\\n [ nan]]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Big Learning Rate\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)  # Learning R0ate를 1.5로 크게 설정 \n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "\n",
    "'''\n",
    "0 Cost:  2.45533e+12\n",
    "Prediction:\n",
    " [[-1104436.375]\n",
    " [-2224342.75 ]\n",
    " [-1749606.75 ]\n",
    " [-1226179.375]\n",
    " [-1445287.125]\n",
    " [-1457459.5  ]\n",
    " [-1335740.5  ]\n",
    " [-1700924.625]]\n",
    "1 Cost:  2.69762e+27\n",
    "Prediction:\n",
    " [[  3.66371490e+13]\n",
    " [  7.37543360e+13]\n",
    " [  5.80198785e+13]\n",
    " [  4.06716290e+13]\n",
    " [  4.79336847e+13]\n",
    " [  4.83371348e+13]\n",
    " [  4.43026590e+13]\n",
    " [  5.64060907e+13]]\n",
    "2 Cost:  inf\n",
    "Prediction:\n",
    " [[ -1.21438790e+21]\n",
    " [ -2.44468702e+21]\n",
    " [ -1.92314724e+21]\n",
    " [ -1.34811610e+21]\n",
    " [ -1.58882674e+21]\n",
    " [ -1.60219962e+21]\n",
    " [ -1.46847142e+21]\n",
    " [ -1.86965602e+21]]\n",
    "3 Cost:  inf\n",
    "Prediction:\n",
    " [[  4.02525216e+28]\n",
    " [  8.10324465e+28]\n",
    " [  6.37453079e+28]\n",
    " [  4.46851237e+28]\n",
    " [  5.26638074e+28]\n",
    " [  5.31070676e+28]\n",
    " [  4.86744608e+28]\n",
    " [  6.19722623e+28]]\n",
    "4 Cost:  inf\n",
    "Prediction:\n",
    " [[ -1.33422428e+36]\n",
    " [ -2.68593010e+36]\n",
    " [ -2.11292430e+36]\n",
    " [ -1.48114879e+36]\n",
    " [ -1.74561303e+36]\n",
    " [ -1.76030542e+36]\n",
    " [ -1.61338091e+36]\n",
    " [ -2.05415459e+36]]\n",
    "5 Cost:  inf                 # 값이 발산, 무한대로 되어버림. \n",
    "Prediction:\n",
    " [[ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]]\n",
    "6 Cost:  nan                # Learning Rate 값이 발산, 학습 포기 \n",
    "Prediction:\n",
    " [[ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  6.008363 \n",
      "Prediction:\n",
      " [[-2.787303  ]\n",
      " [-2.304973  ]\n",
      " [-1.9524162 ]\n",
      " [-1.5445647 ]\n",
      " [-1.8520749 ]\n",
      " [-1.818827  ]\n",
      " [-1.0787338 ]\n",
      " [-0.97747326]]\n",
      "1 Cost:  6.00791 \n",
      "Prediction:\n",
      " [[-2.7871776 ]\n",
      " [-2.304853  ]\n",
      " [-1.9523162 ]\n",
      " [-1.5444871 ]\n",
      " [-1.851983  ]\n",
      " [-1.8187382 ]\n",
      " [-1.078675  ]\n",
      " [-0.97741604]]\n",
      "2 Cost:  6.007457 \n",
      "Prediction:\n",
      " [[-2.7870522]\n",
      " [-2.304733 ]\n",
      " [-1.9522161]\n",
      " [-1.5444096]\n",
      " [-1.851891 ]\n",
      " [-1.8186496]\n",
      " [-1.0786163]\n",
      " [-0.9773588]]\n",
      "3 Cost:  6.007004 \n",
      "Prediction:\n",
      " [[-2.786927 ]\n",
      " [-2.304613 ]\n",
      " [-1.9521161]\n",
      " [-1.5443321]\n",
      " [-1.8517991]\n",
      " [-1.8185608]\n",
      " [-1.0785575]\n",
      " [-0.9773016]]\n",
      "4 Cost:  6.0065517 \n",
      "Prediction:\n",
      " [[-2.7868018]\n",
      " [-2.3044932]\n",
      " [-1.9520161]\n",
      " [-1.5442545]\n",
      " [-1.8517072]\n",
      " [-1.8184723]\n",
      " [-1.0784987]\n",
      " [-0.9772444]]\n",
      "5 Cost:  6.0060987 \n",
      "Prediction:\n",
      " [[-2.7866764]\n",
      " [-2.3043733]\n",
      " [-1.9519161]\n",
      " [-1.544177 ]\n",
      " [-1.8516154]\n",
      " [-1.8183836]\n",
      " [-1.07844  ]\n",
      " [-0.9771871]]\n",
      "6 Cost:  6.0056467 \n",
      "Prediction:\n",
      " [[-2.786551 ]\n",
      " [-2.3042533]\n",
      " [-1.9518161]\n",
      " [-1.5440996]\n",
      " [-1.8515234]\n",
      " [-1.818295 ]\n",
      " [-1.0783813]\n",
      " [-0.9771299]]\n",
      "7 Cost:  6.0051937 \n",
      "Prediction:\n",
      " [[-2.7864256 ]\n",
      " [-2.3041334 ]\n",
      " [-1.9517161 ]\n",
      " [-1.5440221 ]\n",
      " [-1.8514315 ]\n",
      " [-1.8182062 ]\n",
      " [-1.0783225 ]\n",
      " [-0.97707266]]\n",
      "8 Cost:  6.004741 \n",
      "Prediction:\n",
      " [[-2.7863004 ]\n",
      " [-2.3040135 ]\n",
      " [-1.951616  ]\n",
      " [-1.5439446 ]\n",
      " [-1.8513396 ]\n",
      " [-1.8181176 ]\n",
      " [-1.0782638 ]\n",
      " [-0.97701544]]\n",
      "9 Cost:  6.004288 \n",
      "Prediction:\n",
      " [[-2.7861753 ]\n",
      " [-2.3038936 ]\n",
      " [-1.951516  ]\n",
      " [-1.5438671 ]\n",
      " [-1.8512478 ]\n",
      " [-1.8180289 ]\n",
      " [-1.078205  ]\n",
      " [-0.97695816]]\n",
      "10 Cost:  6.0038357 \n",
      "Prediction:\n",
      " [[-2.7860498 ]\n",
      " [-2.3037734 ]\n",
      " [-1.951416  ]\n",
      " [-1.5437895 ]\n",
      " [-1.8511558 ]\n",
      " [-1.8179402 ]\n",
      " [-1.0781462 ]\n",
      " [-0.97690094]]\n",
      "11 Cost:  6.003383 \n",
      "Prediction:\n",
      " [[-2.7859244]\n",
      " [-2.3036537]\n",
      " [-1.951316 ]\n",
      " [-1.5437121]\n",
      " [-1.851064 ]\n",
      " [-1.8178515]\n",
      " [-1.0780874]\n",
      " [-0.9768437]]\n",
      "12 Cost:  6.0029306 \n",
      "Prediction:\n",
      " [[-2.785799 ]\n",
      " [-2.3035336]\n",
      " [-1.9512161]\n",
      " [-1.5436345]\n",
      " [-1.850972 ]\n",
      " [-1.8177629]\n",
      " [-1.0780287]\n",
      " [-0.9767865]]\n",
      "13 Cost:  6.002478 \n",
      "Prediction:\n",
      " [[-2.7856739]\n",
      " [-2.3034139]\n",
      " [-1.951116 ]\n",
      " [-1.543557 ]\n",
      " [-1.8508801]\n",
      " [-1.8176742]\n",
      " [-1.07797  ]\n",
      " [-0.9767293]]\n",
      "14 Cost:  6.0020256 \n",
      "Prediction:\n",
      " [[-2.7855487 ]\n",
      " [-2.3032937 ]\n",
      " [-1.9510161 ]\n",
      " [-1.5434796 ]\n",
      " [-1.8507882 ]\n",
      " [-1.8175855 ]\n",
      " [-1.0779113 ]\n",
      " [-0.97667205]]\n",
      "15 Cost:  6.0015736 \n",
      "Prediction:\n",
      " [[-2.7854233]\n",
      " [-2.303174 ]\n",
      " [-1.950916 ]\n",
      " [-1.543402 ]\n",
      " [-1.8506963]\n",
      " [-1.8174968]\n",
      " [-1.0778525]\n",
      " [-0.9766148]]\n",
      "16 Cost:  6.0011206 \n",
      "Prediction:\n",
      " [[-2.7852979 ]\n",
      " [-2.3030539 ]\n",
      " [-1.950816  ]\n",
      " [-1.5433245 ]\n",
      " [-1.8506044 ]\n",
      " [-1.8174081 ]\n",
      " [-1.0777937 ]\n",
      " [-0.97655755]]\n",
      "17 Cost:  6.0006685 \n",
      "Prediction:\n",
      " [[-2.7851727]\n",
      " [-2.3029342]\n",
      " [-1.950716 ]\n",
      " [-1.543247 ]\n",
      " [-1.8505126]\n",
      " [-1.8173195]\n",
      " [-1.0777351]\n",
      " [-0.9765004]]\n",
      "18 Cost:  6.0002165 \n",
      "Prediction:\n",
      " [[-2.7850475 ]\n",
      " [-2.3028145 ]\n",
      " [-1.9506162 ]\n",
      " [-1.5431696 ]\n",
      " [-1.8504207 ]\n",
      " [-1.8172309 ]\n",
      " [-1.0776763 ]\n",
      " [-0.97644323]]\n",
      "19 Cost:  5.999764 \n",
      "Prediction:\n",
      " [[-2.7849221]\n",
      " [-2.3026943]\n",
      " [-1.9505162]\n",
      " [-1.5430923]\n",
      " [-1.8503289]\n",
      " [-1.8171422]\n",
      " [-1.0776176]\n",
      " [-0.9763861]]\n",
      "20 Cost:  5.999312 \n",
      "Prediction:\n",
      " [[-2.784797  ]\n",
      " [-2.3025746 ]\n",
      " [-1.9504163 ]\n",
      " [-1.5430148 ]\n",
      " [-1.8502371 ]\n",
      " [-1.8170537 ]\n",
      " [-1.077559  ]\n",
      " [-0.97632885]]\n",
      "21 Cost:  5.9988604 \n",
      "Prediction:\n",
      " [[-2.7846718 ]\n",
      " [-2.3024547 ]\n",
      " [-1.9503164 ]\n",
      " [-1.5429373 ]\n",
      " [-1.8501452 ]\n",
      " [-1.8169651 ]\n",
      " [-1.0775002 ]\n",
      " [-0.97627175]]\n",
      "22 Cost:  5.998408 \n",
      "Prediction:\n",
      " [[-2.7845464]\n",
      " [-2.3023348]\n",
      " [-1.9502164]\n",
      " [-1.5428599]\n",
      " [-1.8500534]\n",
      " [-1.8168764]\n",
      " [-1.0774416]\n",
      " [-0.9762145]]\n",
      "23 Cost:  5.997956 \n",
      "Prediction:\n",
      " [[-2.7844212 ]\n",
      " [-2.3022149 ]\n",
      " [-1.9501164 ]\n",
      " [-1.5427823 ]\n",
      " [-1.8499615 ]\n",
      " [-1.8167878 ]\n",
      " [-1.0773829 ]\n",
      " [-0.97615737]]\n",
      "24 Cost:  5.997504 \n",
      "Prediction:\n",
      " [[-2.784296 ]\n",
      " [-2.3020952]\n",
      " [-1.9500166]\n",
      " [-1.542705 ]\n",
      " [-1.8498697]\n",
      " [-1.8166993]\n",
      " [-1.0773242]\n",
      " [-0.9761002]]\n",
      "25 Cost:  5.9970517 \n",
      "Prediction:\n",
      " [[-2.7841709 ]\n",
      " [-2.3019753 ]\n",
      " [-1.9499166 ]\n",
      " [-1.5426276 ]\n",
      " [-1.8497779 ]\n",
      " [-1.8166106 ]\n",
      " [-1.0772655 ]\n",
      " [-0.97604305]]\n",
      "26 Cost:  5.9965997 \n",
      "Prediction:\n",
      " [[-2.7840457]\n",
      " [-2.3018553]\n",
      " [-1.9498167]\n",
      " [-1.5425501]\n",
      " [-1.8496861]\n",
      " [-1.816522 ]\n",
      " [-1.0772069]\n",
      " [-0.9759859]]\n",
      "27 Cost:  5.9961476 \n",
      "Prediction:\n",
      " [[-2.7839203]\n",
      " [-2.3017354]\n",
      " [-1.9497168]\n",
      " [-1.5424726]\n",
      " [-1.8495942]\n",
      " [-1.8164334]\n",
      " [-1.0771482]\n",
      " [-0.9759287]]\n",
      "28 Cost:  5.995695 \n",
      "Prediction:\n",
      " [[-2.7837949 ]\n",
      " [-2.3016157 ]\n",
      " [-1.9496168 ]\n",
      " [-1.5423952 ]\n",
      " [-1.8495024 ]\n",
      " [-1.8163447 ]\n",
      " [-1.0770894 ]\n",
      " [-0.97587156]]\n",
      "29 Cost:  5.995244 \n",
      "Prediction:\n",
      " [[-2.7836697]\n",
      " [-2.3014958]\n",
      " [-1.9495168]\n",
      " [-1.5423179]\n",
      " [-1.8494105]\n",
      " [-1.8162562]\n",
      " [-1.0770308]\n",
      " [-0.9758144]]\n",
      "30 Cost:  5.994792 \n",
      "Prediction:\n",
      " [[-2.7835445 ]\n",
      " [-2.3013759 ]\n",
      " [-1.949417  ]\n",
      " [-1.5422404 ]\n",
      " [-1.8493187 ]\n",
      " [-1.8161676 ]\n",
      " [-1.0769721 ]\n",
      " [-0.97575724]]\n",
      "31 Cost:  5.99434 \n",
      "Prediction:\n",
      " [[-2.7834194]\n",
      " [-2.3012562]\n",
      " [-1.949317 ]\n",
      " [-1.5421629]\n",
      " [-1.849227 ]\n",
      " [-1.8160789]\n",
      " [-1.0769134]\n",
      " [-0.9757   ]]\n",
      "32 Cost:  5.9938884 \n",
      "Prediction:\n",
      " [[-2.7832942 ]\n",
      " [-2.3011365 ]\n",
      " [-1.9492171 ]\n",
      " [-1.5420854 ]\n",
      " [-1.8491352 ]\n",
      " [-1.8159904 ]\n",
      " [-1.0768547 ]\n",
      " [-0.97564286]]\n",
      "33 Cost:  5.993436 \n",
      "Prediction:\n",
      " [[-2.7831688]\n",
      " [-2.3010163]\n",
      " [-1.9491172]\n",
      " [-1.5420079]\n",
      " [-1.8490433]\n",
      " [-1.8159018]\n",
      " [-1.076796 ]\n",
      " [-0.9755857]]\n",
      "34 Cost:  5.992985 \n",
      "Prediction:\n",
      " [[-2.7830436 ]\n",
      " [-2.3008966 ]\n",
      " [-1.9490173 ]\n",
      " [-1.5419307 ]\n",
      " [-1.8489515 ]\n",
      " [-1.8158132 ]\n",
      " [-1.0767374 ]\n",
      " [-0.97552854]]\n",
      "35 Cost:  5.9925327 \n",
      "Prediction:\n",
      " [[-2.7829185]\n",
      " [-2.3007767]\n",
      " [-1.9489174]\n",
      " [-1.5418532]\n",
      " [-1.8488597]\n",
      " [-1.8157246]\n",
      " [-1.0766786]\n",
      " [-0.9754714]]\n",
      "36 Cost:  5.992081 \n",
      "Prediction:\n",
      " [[-2.7827933]\n",
      " [-2.300657 ]\n",
      " [-1.9488175]\n",
      " [-1.5417757]\n",
      " [-1.8487678]\n",
      " [-1.8156359]\n",
      " [-1.07662  ]\n",
      " [-0.9754142]]\n",
      "37 Cost:  5.9916296 \n",
      "Prediction:\n",
      " [[-2.782668  ]\n",
      " [-2.3005373 ]\n",
      " [-1.9487176 ]\n",
      " [-1.5416983 ]\n",
      " [-1.848676  ]\n",
      " [-1.8155475 ]\n",
      " [-1.0765613 ]\n",
      " [-0.97535706]]\n",
      "38 Cost:  5.9911776 \n",
      "Prediction:\n",
      " [[-2.782543 ]\n",
      " [-2.3004174]\n",
      " [-1.9486177]\n",
      " [-1.541621 ]\n",
      " [-1.8485842]\n",
      " [-1.8154588]\n",
      " [-1.0765026]\n",
      " [-0.9752999]]\n",
      "39 Cost:  5.990726 \n",
      "Prediction:\n",
      " [[-2.7824178 ]\n",
      " [-2.3002977 ]\n",
      " [-1.9485178 ]\n",
      " [-1.5415435 ]\n",
      " [-1.8484924 ]\n",
      " [-1.8153703 ]\n",
      " [-1.0764439 ]\n",
      " [-0.97524273]]\n",
      "40 Cost:  5.990275 \n",
      "Prediction:\n",
      " [[-2.7822928 ]\n",
      " [-2.3001778 ]\n",
      " [-1.9484179 ]\n",
      " [-1.541466  ]\n",
      " [-1.8484006 ]\n",
      " [-1.8152816 ]\n",
      " [-1.0763853 ]\n",
      " [-0.97518563]]\n",
      "41 Cost:  5.9898233 \n",
      "Prediction:\n",
      " [[-2.7821677]\n",
      " [-2.3000581]\n",
      " [-1.948318 ]\n",
      " [-1.5413888]\n",
      " [-1.8483088]\n",
      " [-1.8151932]\n",
      " [-1.0763266]\n",
      " [-0.9751284]]\n",
      "42 Cost:  5.9893713 \n",
      "Prediction:\n",
      " [[-2.7820425]\n",
      " [-2.2999382]\n",
      " [-1.9482181]\n",
      " [-1.5413113]\n",
      " [-1.848217 ]\n",
      " [-1.8151045]\n",
      " [-1.0762678]\n",
      " [-0.9750713]]\n",
      "43 Cost:  5.9889197 \n",
      "Prediction:\n",
      " [[-2.781917  ]\n",
      " [-2.2998185 ]\n",
      " [-1.9481182 ]\n",
      " [-1.5412338 ]\n",
      " [-1.8481252 ]\n",
      " [-1.8150159 ]\n",
      " [-1.0762092 ]\n",
      " [-0.97501415]]\n",
      "44 Cost:  5.988469 \n",
      "Prediction:\n",
      " [[-2.7817922]\n",
      " [-2.2996988]\n",
      " [-1.9480184]\n",
      " [-1.5411564]\n",
      " [-1.8480334]\n",
      " [-1.8149273]\n",
      " [-1.0761505]\n",
      " [-0.974957 ]]\n",
      "45 Cost:  5.9880176 \n",
      "Prediction:\n",
      " [[-2.781667 ]\n",
      " [-2.299579 ]\n",
      " [-1.9479184]\n",
      " [-1.541079 ]\n",
      " [-1.8479416]\n",
      " [-1.8148388]\n",
      " [-1.0760919]\n",
      " [-0.9748998]]\n",
      "46 Cost:  5.987566 \n",
      "Prediction:\n",
      " [[-2.7815418 ]\n",
      " [-2.2994592 ]\n",
      " [-1.9478186 ]\n",
      " [-1.5410016 ]\n",
      " [-1.84785   ]\n",
      " [-1.8147502 ]\n",
      " [-1.0760331 ]\n",
      " [-0.97484267]]\n",
      "47 Cost:  5.987114 \n",
      "Prediction:\n",
      " [[-2.7814164]\n",
      " [-2.2993393]\n",
      " [-1.9477186]\n",
      " [-1.5409242]\n",
      " [-1.847758 ]\n",
      " [-1.8146617]\n",
      " [-1.0759745]\n",
      " [-0.9747855]]\n",
      "48 Cost:  5.986663 \n",
      "Prediction:\n",
      " [[-2.7812915 ]\n",
      " [-2.2992196 ]\n",
      " [-1.9476188 ]\n",
      " [-1.5408468 ]\n",
      " [-1.8476663 ]\n",
      " [-1.814573  ]\n",
      " [-1.0759158 ]\n",
      " [-0.97472835]]\n",
      "49 Cost:  5.986212 \n",
      "Prediction:\n",
      " [[-2.7811663]\n",
      " [-2.2991   ]\n",
      " [-1.9475191]\n",
      " [-1.5407693]\n",
      " [-1.8475745]\n",
      " [-1.8144846]\n",
      " [-1.0758572]\n",
      " [-0.9746712]]\n",
      "50 Cost:  5.9857607 \n",
      "Prediction:\n",
      " [[-2.7810411 ]\n",
      " [-2.29898   ]\n",
      " [-1.9474192 ]\n",
      " [-1.5406921 ]\n",
      " [-1.8474827 ]\n",
      " [-1.814396  ]\n",
      " [-1.0757985 ]\n",
      " [-0.97461414]]\n",
      "51 Cost:  5.9853096 \n",
      "Prediction:\n",
      " [[-2.780916  ]\n",
      " [-2.2988605 ]\n",
      " [-1.9473193 ]\n",
      " [-1.5406147 ]\n",
      " [-1.8473911 ]\n",
      " [-1.8143075 ]\n",
      " [-1.0757399 ]\n",
      " [-0.97455704]]\n",
      "52 Cost:  5.9848585 \n",
      "Prediction:\n",
      " [[-2.780791  ]\n",
      " [-2.2987406 ]\n",
      " [-1.9472196 ]\n",
      " [-1.5405374 ]\n",
      " [-1.8472993 ]\n",
      " [-1.814219  ]\n",
      " [-1.0756813 ]\n",
      " [-0.97449994]]\n",
      "53 Cost:  5.9844074 \n",
      "Prediction:\n",
      " [[-2.7806659 ]\n",
      " [-2.298621  ]\n",
      " [-1.9471197 ]\n",
      " [-1.5404599 ]\n",
      " [-1.8472075 ]\n",
      " [-1.8141305 ]\n",
      " [-1.0756227 ]\n",
      " [-0.97444284]]\n",
      "54 Cost:  5.983957 \n",
      "Prediction:\n",
      " [[-2.7805407 ]\n",
      " [-2.2985013 ]\n",
      " [-1.9470198 ]\n",
      " [-1.5403826 ]\n",
      " [-1.847116  ]\n",
      " [-1.8140421 ]\n",
      " [-1.075564  ]\n",
      " [-0.97438574]]\n",
      "55 Cost:  5.983506 \n",
      "Prediction:\n",
      " [[-2.7804158 ]\n",
      " [-2.2983816 ]\n",
      " [-1.9469202 ]\n",
      " [-1.5403054 ]\n",
      " [-1.8470242 ]\n",
      " [-1.8139536 ]\n",
      " [-1.0755055 ]\n",
      " [-0.97432864]]\n",
      "56 Cost:  5.9830546 \n",
      "Prediction:\n",
      " [[-2.7802906 ]\n",
      " [-2.298262  ]\n",
      " [-1.9468203 ]\n",
      " [-1.5402279 ]\n",
      " [-1.8469324 ]\n",
      " [-1.813865  ]\n",
      " [-1.0754468 ]\n",
      " [-0.97427154]]\n",
      "57 Cost:  5.982604 \n",
      "Prediction:\n",
      " [[-2.7801657 ]\n",
      " [-2.2981422 ]\n",
      " [-1.9467204 ]\n",
      " [-1.5401506 ]\n",
      " [-1.8468407 ]\n",
      " [-1.8137765 ]\n",
      " [-1.0753882 ]\n",
      " [-0.97421443]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 Cost:  5.982153 \n",
      "Prediction:\n",
      " [[-2.7800405]\n",
      " [-2.2980225]\n",
      " [-1.9466207]\n",
      " [-1.5400733]\n",
      " [-1.8467491]\n",
      " [-1.813688 ]\n",
      " [-1.0753295]\n",
      " [-0.9741574]]\n",
      "59 Cost:  5.9817023 \n",
      "Prediction:\n",
      " [[-2.7799153]\n",
      " [-2.2979028]\n",
      " [-1.9465208]\n",
      " [-1.5399959]\n",
      " [-1.8466573]\n",
      " [-1.8135996]\n",
      " [-1.075271 ]\n",
      " [-0.9741003]]\n",
      "60 Cost:  5.9812517 \n",
      "Prediction:\n",
      " [[-2.7797904]\n",
      " [-2.2977831]\n",
      " [-1.9464211]\n",
      " [-1.5399185]\n",
      " [-1.8465655]\n",
      " [-1.8135109]\n",
      " [-1.0752124]\n",
      " [-0.9740432]]\n",
      "61 Cost:  5.9808006 \n",
      "Prediction:\n",
      " [[-2.7796652]\n",
      " [-2.2976635]\n",
      " [-1.9463212]\n",
      " [-1.5398412]\n",
      " [-1.8464739]\n",
      " [-1.8134224]\n",
      " [-1.0751537]\n",
      " [-0.9739861]]\n",
      "62 Cost:  5.9803495 \n",
      "Prediction:\n",
      " [[-2.77954   ]\n",
      " [-2.2975438 ]\n",
      " [-1.9462214 ]\n",
      " [-1.5397639 ]\n",
      " [-1.8463821 ]\n",
      " [-1.813334  ]\n",
      " [-1.0750952 ]\n",
      " [-0.97392905]]\n",
      "63 Cost:  5.9798985 \n",
      "Prediction:\n",
      " [[-2.779415  ]\n",
      " [-2.2974238 ]\n",
      " [-1.9461217 ]\n",
      " [-1.5396866 ]\n",
      " [-1.8462903 ]\n",
      " [-1.8132455 ]\n",
      " [-1.0750365 ]\n",
      " [-0.97387195]]\n",
      "64 Cost:  5.9794483 \n",
      "Prediction:\n",
      " [[-2.77929   ]\n",
      " [-2.2973044 ]\n",
      " [-1.9460218 ]\n",
      " [-1.5396092 ]\n",
      " [-1.8461988 ]\n",
      " [-1.8131571 ]\n",
      " [-1.0749779 ]\n",
      " [-0.97381485]]\n",
      "65 Cost:  5.978998 \n",
      "Prediction:\n",
      " [[-2.779165  ]\n",
      " [-2.2971847 ]\n",
      " [-1.9459221 ]\n",
      " [-1.539532  ]\n",
      " [-1.846107  ]\n",
      " [-1.8130686 ]\n",
      " [-1.0749192 ]\n",
      " [-0.97375774]]\n",
      "66 Cost:  5.978547 \n",
      "Prediction:\n",
      " [[-2.7790399 ]\n",
      " [-2.297065  ]\n",
      " [-1.9458222 ]\n",
      " [-1.5394546 ]\n",
      " [-1.8460152 ]\n",
      " [-1.81298   ]\n",
      " [-1.0748607 ]\n",
      " [-0.97370064]]\n",
      "67 Cost:  5.978096 \n",
      "Prediction:\n",
      " [[-2.7789147]\n",
      " [-2.296945 ]\n",
      " [-1.9457223]\n",
      " [-1.5393772]\n",
      " [-1.8459237]\n",
      " [-1.8128915]\n",
      " [-1.074802 ]\n",
      " [-0.9736436]]\n",
      "68 Cost:  5.9776454 \n",
      "Prediction:\n",
      " [[-2.7787898]\n",
      " [-2.2968254]\n",
      " [-1.9456227]\n",
      " [-1.5393   ]\n",
      " [-1.8458319]\n",
      " [-1.812803 ]\n",
      " [-1.0747435]\n",
      " [-0.9735865]]\n",
      "69 Cost:  5.977195 \n",
      "Prediction:\n",
      " [[-2.7786646]\n",
      " [-2.296706 ]\n",
      " [-1.9455228]\n",
      " [-1.5392225]\n",
      " [-1.8457401]\n",
      " [-1.8127146]\n",
      " [-1.0746849]\n",
      " [-0.9735294]]\n",
      "70 Cost:  5.9767447 \n",
      "Prediction:\n",
      " [[-2.7785397]\n",
      " [-2.2965863]\n",
      " [-1.9454231]\n",
      " [-1.5391452]\n",
      " [-1.8456484]\n",
      " [-1.8126261]\n",
      " [-1.0746262]\n",
      " [-0.9734723]]\n",
      "71 Cost:  5.976293 \n",
      "Prediction:\n",
      " [[-2.7784145 ]\n",
      " [-2.2964664 ]\n",
      " [-1.9453232 ]\n",
      " [-1.5390679 ]\n",
      " [-1.8455567 ]\n",
      " [-1.8125374 ]\n",
      " [-1.0745676 ]\n",
      " [-0.97341526]]\n",
      "72 Cost:  5.975842 \n",
      "Prediction:\n",
      " [[-2.7782893 ]\n",
      " [-2.2963467 ]\n",
      " [-1.9452233 ]\n",
      " [-1.5389905 ]\n",
      " [-1.8454651 ]\n",
      " [-1.812449  ]\n",
      " [-1.074509  ]\n",
      " [-0.97335815]]\n",
      "73 Cost:  5.9753923 \n",
      "Prediction:\n",
      " [[-2.7781644 ]\n",
      " [-2.296227  ]\n",
      " [-1.9451237 ]\n",
      " [-1.5389132 ]\n",
      " [-1.8453733 ]\n",
      " [-1.8123605 ]\n",
      " [-1.0744504 ]\n",
      " [-0.97330105]]\n",
      "74 Cost:  5.9749417 \n",
      "Prediction:\n",
      " [[-2.7780392 ]\n",
      " [-2.2961075 ]\n",
      " [-1.9450238 ]\n",
      " [-1.5388358 ]\n",
      " [-1.8452816 ]\n",
      " [-1.8122721 ]\n",
      " [-1.0743918 ]\n",
      " [-0.97324395]]\n",
      "75 Cost:  5.974491 \n",
      "Prediction:\n",
      " [[-2.777914 ]\n",
      " [-2.2959878]\n",
      " [-1.9449241]\n",
      " [-1.5387585]\n",
      " [-1.8451899]\n",
      " [-1.8121836]\n",
      " [-1.0743332]\n",
      " [-0.9731869]]\n",
      "76 Cost:  5.974041 \n",
      "Prediction:\n",
      " [[-2.777789 ]\n",
      " [-2.2958682]\n",
      " [-1.9448242]\n",
      " [-1.5386813]\n",
      " [-1.8450983]\n",
      " [-1.8120952]\n",
      " [-1.0742745]\n",
      " [-0.9731298]]\n",
      "77 Cost:  5.9735904 \n",
      "Prediction:\n",
      " [[-2.7776642]\n",
      " [-2.2957485]\n",
      " [-1.9447246]\n",
      " [-1.5386038]\n",
      " [-1.8450065]\n",
      " [-1.8120067]\n",
      " [-1.074216 ]\n",
      " [-0.9730727]]\n",
      "78 Cost:  5.97314 \n",
      "Prediction:\n",
      " [[-2.777539  ]\n",
      " [-2.2956288 ]\n",
      " [-1.9446247 ]\n",
      " [-1.5385265 ]\n",
      " [-1.8449148 ]\n",
      " [-1.8119181 ]\n",
      " [-1.0741574 ]\n",
      " [-0.97301567]]\n",
      "79 Cost:  5.9726896 \n",
      "Prediction:\n",
      " [[-2.7774138 ]\n",
      " [-2.295509  ]\n",
      " [-1.944525  ]\n",
      " [-1.5384492 ]\n",
      " [-1.8448231 ]\n",
      " [-1.8118297 ]\n",
      " [-1.0740987 ]\n",
      " [-0.97295856]]\n",
      "80 Cost:  5.972239 \n",
      "Prediction:\n",
      " [[-2.777289  ]\n",
      " [-2.2953894 ]\n",
      " [-1.9444251 ]\n",
      " [-1.5383718 ]\n",
      " [-1.8447313 ]\n",
      " [-1.8117411 ]\n",
      " [-1.0740402 ]\n",
      " [-0.97290146]]\n",
      "81 Cost:  5.9717884 \n",
      "Prediction:\n",
      " [[-2.777164  ]\n",
      " [-2.2952697 ]\n",
      " [-1.9443254 ]\n",
      " [-1.5382946 ]\n",
      " [-1.8446398 ]\n",
      " [-1.8116527 ]\n",
      " [-1.0739815 ]\n",
      " [-0.97284436]]\n",
      "82 Cost:  5.9713383 \n",
      "Prediction:\n",
      " [[-2.7770388]\n",
      " [-2.29515  ]\n",
      " [-1.9442255]\n",
      " [-1.5382172]\n",
      " [-1.844548 ]\n",
      " [-1.8115642]\n",
      " [-1.0739229]\n",
      " [-0.9727873]]\n",
      "83 Cost:  5.9708877 \n",
      "Prediction:\n",
      " [[-2.7769136]\n",
      " [-2.2950304]\n",
      " [-1.9441257]\n",
      " [-1.5381398]\n",
      " [-1.8444563]\n",
      " [-1.8114758]\n",
      " [-1.0738643]\n",
      " [-0.9727302]]\n",
      "84 Cost:  5.9704375 \n",
      "Prediction:\n",
      " [[-2.7767887]\n",
      " [-2.294911 ]\n",
      " [-1.9440261]\n",
      " [-1.5380626]\n",
      " [-1.8443646]\n",
      " [-1.8113873]\n",
      " [-1.0738058]\n",
      " [-0.9726732]]\n",
      "85 Cost:  5.9699874 \n",
      "Prediction:\n",
      " [[-2.7766638]\n",
      " [-2.2947912]\n",
      " [-1.9439263]\n",
      " [-1.5379853]\n",
      " [-1.844273 ]\n",
      " [-1.8112988]\n",
      " [-1.0737472]\n",
      " [-0.9726162]]\n",
      "86 Cost:  5.9695377 \n",
      "Prediction:\n",
      " [[-2.7765388 ]\n",
      " [-2.2946715 ]\n",
      " [-1.9438267 ]\n",
      " [-1.5379081 ]\n",
      " [-1.8441813 ]\n",
      " [-1.8112104 ]\n",
      " [-1.0736887 ]\n",
      " [-0.97255915]]\n",
      "87 Cost:  5.9690876 \n",
      "Prediction:\n",
      " [[-2.7764137]\n",
      " [-2.294552 ]\n",
      " [-1.9437268]\n",
      " [-1.5378308]\n",
      " [-1.8440897]\n",
      " [-1.8111221]\n",
      " [-1.0736301]\n",
      " [-0.9725022]]\n",
      "88 Cost:  5.9686384 \n",
      "Prediction:\n",
      " [[-2.7762887 ]\n",
      " [-2.2944324 ]\n",
      " [-1.9436272 ]\n",
      " [-1.5377536 ]\n",
      " [-1.8439981 ]\n",
      " [-1.8110337 ]\n",
      " [-1.0735716 ]\n",
      " [-0.97244513]]\n",
      "89 Cost:  5.968188 \n",
      "Prediction:\n",
      " [[-2.7761638 ]\n",
      " [-2.2943127 ]\n",
      " [-1.9435275 ]\n",
      " [-1.5376763 ]\n",
      " [-1.8439064 ]\n",
      " [-1.8109453 ]\n",
      " [-1.073513  ]\n",
      " [-0.97238815]]\n",
      "90 Cost:  5.9677377 \n",
      "Prediction:\n",
      " [[-2.7760386]\n",
      " [-2.2941933]\n",
      " [-1.9434277]\n",
      " [-1.5375991]\n",
      " [-1.8438148]\n",
      " [-1.8108568]\n",
      " [-1.0734545]\n",
      " [-0.9723311]]\n",
      "91 Cost:  5.9672875 \n",
      "Prediction:\n",
      " [[-2.7759137 ]\n",
      " [-2.2940736 ]\n",
      " [-1.9433281 ]\n",
      " [-1.5375217 ]\n",
      " [-1.8437232 ]\n",
      " [-1.8107684 ]\n",
      " [-1.073396  ]\n",
      " [-0.97227407]]\n",
      "92 Cost:  5.966838 \n",
      "Prediction:\n",
      " [[-2.7757888]\n",
      " [-2.293954 ]\n",
      " [-1.9432284]\n",
      " [-1.5374445]\n",
      " [-1.8436315]\n",
      " [-1.8106799]\n",
      " [-1.0733374]\n",
      " [-0.9722171]]\n",
      "93 Cost:  5.966388 \n",
      "Prediction:\n",
      " [[-2.7756639 ]\n",
      " [-2.2938344 ]\n",
      " [-1.9431286 ]\n",
      " [-1.5373672 ]\n",
      " [-1.84354   ]\n",
      " [-1.8105916 ]\n",
      " [-1.0732789 ]\n",
      " [-0.97216004]]\n",
      "94 Cost:  5.9659386 \n",
      "Prediction:\n",
      " [[-2.775539 ]\n",
      " [-2.2937148]\n",
      " [-1.9430289]\n",
      " [-1.5372899]\n",
      " [-1.8434483]\n",
      " [-1.8105031]\n",
      " [-1.0732204]\n",
      " [-0.972103 ]]\n",
      "95 Cost:  5.9654884 \n",
      "Prediction:\n",
      " [[-2.7754138]\n",
      " [-2.2935953]\n",
      " [-1.9429293]\n",
      " [-1.5372126]\n",
      " [-1.8433566]\n",
      " [-1.8104147]\n",
      " [-1.0731618]\n",
      " [-0.972046 ]]\n",
      "96 Cost:  5.9650383 \n",
      "Prediction:\n",
      " [[-2.7752888]\n",
      " [-2.2934756]\n",
      " [-1.9428295]\n",
      " [-1.5371354]\n",
      " [-1.843265 ]\n",
      " [-1.8103263]\n",
      " [-1.0731032]\n",
      " [-0.971989 ]]\n",
      "97 Cost:  5.964588 \n",
      "Prediction:\n",
      " [[-2.775164 ]\n",
      " [-2.293356 ]\n",
      " [-1.9427297]\n",
      " [-1.5370581]\n",
      " [-1.8431734]\n",
      " [-1.8102379]\n",
      " [-1.0730447]\n",
      " [-0.971932 ]]\n",
      "98 Cost:  5.964139 \n",
      "Prediction:\n",
      " [[-2.7750387 ]\n",
      " [-2.2932365 ]\n",
      " [-1.9426302 ]\n",
      " [-1.5369809 ]\n",
      " [-1.8430818 ]\n",
      " [-1.8101494 ]\n",
      " [-1.0729861 ]\n",
      " [-0.97187495]]\n",
      "99 Cost:  5.9636893 \n",
      "Prediction:\n",
      " [[-2.7749138]\n",
      " [-2.2931168]\n",
      " [-1.9425304]\n",
      " [-1.5369036]\n",
      " [-1.8429902]\n",
      " [-1.810061 ]\n",
      " [-1.0729276]\n",
      " [-0.971818 ]]\n",
      "100 Cost:  5.963239 \n",
      "Prediction:\n",
      " [[-2.7747889]\n",
      " [-2.2929974]\n",
      " [-1.9424306]\n",
      " [-1.5368264]\n",
      " [-1.8428986]\n",
      " [-1.8099726]\n",
      " [-1.0728691]\n",
      " [-0.9717609]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 Cost: 0.15230925 \\nPrediction:\\n [[ 1.6346191 ]\\n [ 0.06613699]\\n [ 0.3500818 ]\\n [ 0.6707252 ]\\n [ 0.61130744]\\n [ 0.61464405]\\n [ 0.23171967]\\n [-0.1372836 ]]\\n1 Cost: 0.15230872 \\nPrediction:\\n [[ 1.634618  ]\\n [ 0.06613836]\\n [ 0.35008252]\\n [ 0.670725  ]\\n [ 0.6113076 ]\\n [ 0.6146443 ]\\n [ 0.23172   ]\\n [-0.13728246]]\\n...\\n99 Cost: 0.1522546 \\nPrediction:\\n [[ 1.6345041 ]\\n [ 0.06627947]\\n [ 0.35014683]\\n [ 0.670706  ]\\n [ 0.6113161 ]\\n [ 0.61466044]\\n [ 0.23175153]\\n [-0.13716647]]\\n100 Cost: 0.15225402 \\nPrediction:\\n [[ 1.6345029 ]\\n [ 0.06628093]\\n [ 0.35014752]\\n [ 0.67070574]\\n [ 0.61131614]\\n [ 0.6146606 ]\\n [ 0.23175186]\\n [-0.13716528]]\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Small Learning Rate\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# very important. It does not work without it.\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)\n",
    "\n",
    "'''\n",
    "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
    " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
    " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
    " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
    " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
    " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
    " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
    " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
    "'''\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost) # Learning rate를 아주 작게 설정 \n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, cost_val, hy_val = sess.run(\n",
    "            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "'''\n",
    "0 Cost: 0.15230925 \n",
    "Prediction:\n",
    " [[ 1.6346191 ]\n",
    " [ 0.06613699]\n",
    " [ 0.3500818 ]\n",
    " [ 0.6707252 ]\n",
    " [ 0.61130744]\n",
    " [ 0.61464405]\n",
    " [ 0.23171967]\n",
    " [-0.1372836 ]]\n",
    "1 Cost: 0.15230872 \n",
    "Prediction:\n",
    " [[ 1.634618  ]\n",
    " [ 0.06613836]\n",
    " [ 0.35008252]\n",
    " [ 0.670725  ]\n",
    " [ 0.6113076 ]\n",
    " [ 0.6146443 ]\n",
    " [ 0.23172   ]\n",
    " [-0.13728246]]\n",
    "...\n",
    "99 Cost: 0.1522546 \n",
    "Prediction:\n",
    " [[ 1.6345041 ]\n",
    " [ 0.06627947]\n",
    " [ 0.35014683]\n",
    " [ 0.670706  ]\n",
    " [ 0.6113161 ]\n",
    " [ 0.61466044]\n",
    " [ 0.23175153]\n",
    " [-0.13716647]]\n",
    "100 Cost: 0.15225402 \n",
    "Prediction:\n",
    " [[ 1.6345029 ]\n",
    " [ 0.06628093]\n",
    " [ 0.35014752]\n",
    " [ 0.67070574]\n",
    " [ 0.61131614]\n",
    " [ 0.6146606 ]\n",
    " [ 0.23175186]\n",
    " [-0.13716528]]                # 학습을 여러번 하여도 Cost 값이 같음. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN(Non-normalized inputs)\n",
    ":해결 --> Nomalized inputs(Min-max scaler), 모든 값들을 Min-max scaler를 통해 정규화를 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
