{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3438986e48e7>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ANACONDA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ANACONDA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ANACONDA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ANACONDA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ANACONDA\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0001, Cost: 5.745170957\n",
      "Epoch: 0002, Cost: 1.780056700\n",
      "Epoch: 0003, Cost: 1.122778628\n",
      "Epoch: 0004, Cost: 0.872012251\n",
      "Epoch: 0005, Cost: 0.738203178\n",
      "Epoch: 0006, Cost: 0.654728890\n",
      "Epoch: 0007, Cost: 0.596023609\n",
      "Epoch: 0008, Cost: 0.552216812\n",
      "Epoch: 0009, Cost: 0.518254966\n",
      "Epoch: 0010, Cost: 0.491113198\n",
      "Epoch: 0011, Cost: 0.468347533\n",
      "Epoch: 0012, Cost: 0.449374352\n",
      "Epoch: 0013, Cost: 0.432675655\n",
      "Epoch: 0014, Cost: 0.418828147\n",
      "Epoch: 0015, Cost: 0.406128929\n",
      "Epoch: 0016, Cost: 0.394982936\n",
      "Epoch: 0017, Cost: 0.385870410\n",
      "Epoch: 0018, Cost: 0.376135572\n",
      "Epoch: 0019, Cost: 0.368269368\n",
      "Epoch: 0020, Cost: 0.361209763\n",
      "Epoch: 0021, Cost: 0.354798134\n",
      "Epoch: 0022, Cost: 0.348525118\n",
      "Epoch: 0023, Cost: 0.342752714\n",
      "Epoch: 0024, Cost: 0.337285907\n",
      "Epoch: 0025, Cost: 0.332443589\n",
      "Epoch: 0026, Cost: 0.327556524\n",
      "Epoch: 0027, Cost: 0.324047218\n",
      "Epoch: 0028, Cost: 0.319670891\n",
      "Epoch: 0029, Cost: 0.315536198\n",
      "Epoch: 0030, Cost: 0.312257744\n",
      "Epoch: 0031, Cost: 0.308550802\n",
      "Epoch: 0032, Cost: 0.305987603\n",
      "Epoch: 0033, Cost: 0.302624448\n",
      "Epoch: 0034, Cost: 0.299895888\n",
      "Epoch: 0035, Cost: 0.297245865\n",
      "Epoch: 0036, Cost: 0.294490162\n",
      "Epoch: 0037, Cost: 0.292061200\n",
      "Epoch: 0038, Cost: 0.290009234\n",
      "Epoch: 0039, Cost: 0.287633514\n",
      "Epoch: 0040, Cost: 0.285644489\n",
      "Epoch: 0041, Cost: 0.283856591\n",
      "Epoch: 0042, Cost: 0.281824805\n",
      "Epoch: 0043, Cost: 0.280098961\n",
      "Epoch: 0044, Cost: 0.278386731\n",
      "Epoch: 0045, Cost: 0.276589549\n",
      "Epoch: 0046, Cost: 0.275093692\n",
      "Epoch: 0047, Cost: 0.273444037\n",
      "Epoch: 0048, Cost: 0.271918674\n",
      "Epoch: 0049, Cost: 0.270640426\n",
      "Epoch: 0050, Cost: 0.269054369\n",
      "Learning Finished!\n",
      "Accuracy: 0.9194\n",
      "Label:  [9]\n",
      "Prediction:  [4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])   # 28*28개의 픽셀이기 때문\n",
    "Y = tf.placeholder(tf.float32, [None, 10])    # 구분하고자 하는것이 10개의 클래스. \n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(\n",
    "            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-3-86acbd759a48>:40: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost = 166.231664942\n",
      "Epoch: 0002 cost = 41.032122905\n",
      "Epoch: 0003 cost = 25.594537339\n",
      "Epoch: 0004 cost = 17.864088863\n",
      "Epoch: 0005 cost = 12.993013247\n",
      "Epoch: 0006 cost = 9.629251075\n",
      "Epoch: 0007 cost = 7.197852673\n",
      "Epoch: 0008 cost = 5.504310540\n",
      "Epoch: 0009 cost = 4.149610768\n",
      "Epoch: 0010 cost = 3.057102702\n",
      "Epoch: 0011 cost = 2.302002925\n",
      "Epoch: 0012 cost = 1.698716345\n",
      "Epoch: 0013 cost = 1.253718311\n",
      "Epoch: 0014 cost = 1.039722997\n",
      "Epoch: 0015 cost = 0.817048254\n",
      "Learning Finished!\n",
      "Accuracy: 0.9375\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers      # Neural Net -> 3개를 쌓음. \n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))   # 처음 shape은 input의 수만큼. \n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))  # 중간 layer들은 각자 알아서. 하지만 직전 output과 다음layer의 input값은 같아야함. \n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))   # output값은 구분하고자 하는 클래스의 수만큼\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    " # 활성화함수 ReLU 사용. \n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST and Xavier(가중치 초기화 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.302479922\n",
      "Epoch: 0002 cost = 0.120086840\n",
      "Epoch: 0003 cost = 0.077895876\n",
      "Epoch: 0004 cost = 0.055617888\n",
      "Epoch: 0005 cost = 0.041345159\n",
      "Epoch: 0006 cost = 0.031439992\n",
      "Epoch: 0007 cost = 0.026323797\n",
      "Epoch: 0008 cost = 0.020913040\n",
      "Epoch: 0009 cost = 0.015036309\n",
      "Epoch: 0010 cost = 0.017349176\n",
      "Epoch: 0011 cost = 0.013732709\n",
      "Epoch: 0012 cost = 0.009120116\n",
      "Epoch: 0013 cost = 0.008479324\n",
      "Epoch: 0014 cost = 0.013354161\n",
      "Epoch: 0015 cost = 0.011646026\n",
      "Learning Finished!\n",
      "Accuracy: 0.977\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 MNIST and Xavier\n",
    "# Initialization 부분을 Xavier로만 바꿈. \n",
    "# 1 Epoch 부터 오차가 적음 == 초기값이 잘 설정되어 있다. \n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())   # Initialization 부분을 Xavier로만 바꿈. \n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
